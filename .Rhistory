}
df3 <- data.frame(cbind(New, count))
print(df3)
#Question2: Show how often does each element in "new" occur in "Pcodes"
#...
New <- vector()
count <- vector()
items <- list(unique(Pcodes))
for (i in 1:length(items)){
New[i] <- items[i]
count[i] <- sum(new == item)
}
df3 <- data.frame(cbind(New, count))
print(df3)
#read the dataset, Warning: This takes several minutes
start.time <- Sys.time()
data = read.fwf("D:/IVAN/6th session/INFO411 - Data Mining and Knowledge Discovery/Lab/Lab 3/info411.dat", widths= c(17,2,17,2,10,5,3,2,2),col.names=c("CardNumber","CardIssue","PIN","gender","DOB","postcode","homecode","enrolment", "payee"), strip.white=TRUE, n=-1)
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
#Lets get some statistics...
print(paste("Number of records read:", length(data$PIN)))
#Compute number of unique Cards
num = length(unique(data$CardNumber))
print(paste("Number of unique cards: ", num))
#Compute number of unique PINs
num = length(unique(data$PIN))
print(paste("Number of unique patients: ", num))
#Now lets have a closer look at some of the attributes. Lets start with the postcodes
Pcodes=data$postcode
#list unique values
uPcodes = unique(Pcodes)
print(paste("Number of postcode areas patients live in: ", length(uPcodes)))
#list number of ocurrences for each unique postcode value
table(uPcodes)
#Get list of Australian postcodes (from the Australian post office)
codes = read.csv("D:/IVAN/6th session/INFO411 - Data Mining and Knowledge Discovery/Lab/Lab 3/valid_postcodes.csv", sep=",")
pcodes = codes$Pcode
#Question1: What does this next line of R-code find?
new <- uPcodes[which(!uPcodes %in% pcodes)]
print(new)
#Question2: Show how often does each element in "new" occur in "Pcodes"
#...
New <- vector()
count <- vector()
for (i in 1:length(new)){
New[i] <- new[i]
count[i] <- sum(Pcodes == item)
}
for (i in 1:length(new)){
New[i] <- new[i]
count[i] <- sum(Pcodes == New)
}
df3 <- data.frame(cbind(New, count))
print(df3)
#read the dataset, Warning: This takes several minutes
start.time <- Sys.time()
data = read.fwf("D:/IVAN/6th session/INFO411 - Data Mining and Knowledge Discovery/Lab/Lab 3/info411.dat", widths= c(17,2,17,2,10,5,3,2,2),col.names=c("CardNumber","CardIssue","PIN","gender","DOB","postcode","homecode","enrolment", "payee"), strip.white=TRUE, n=-1)
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
#Lets get some statistics...
print(paste("Number of records read:", length(data$PIN)))
#Compute number of unique Cards
num = length(unique(data$CardNumber))
print(paste("Number of unique cards: ", num))
#Compute number of unique PINs
num = length(unique(data$PIN))
print(paste("Number of unique patients: ", num))
#Now lets have a closer look at some of the attributes. Lets start with the postcodes
Pcodes=data$postcode
#list unique values
uPcodes = unique(Pcodes)
print(paste("Number of postcode areas patients live in: ", length(uPcodes)))
#list number of ocurrences for each unique postcode value
table(uPcodes)
#Get list of Australian postcodes (from the Australian post office)
codes = read.csv("D:/IVAN/6th session/INFO411 - Data Mining and Knowledge Discovery/Lab/Lab 3/valid_postcodes.csv", sep=",")
pcodes = codes$Pcode
#Question1: What does this next line of R-code find?
new <- uPcodes[which(!uPcodes %in% pcodes)]
print(new)
#Question2: Show how often does each element in "new" occur in "Pcodes"
#...
count <- vector()
for (i in 1:length(new)){
count[i] <- sum(Pcodes == new)
}
df3 <- data.frame(cbind(new, count))
print(df3)
#Question2: Show how often does each element in "new" occur in "Pcodes"
#...
New <- vector()
count <- vector()
for (i in 1:length(new)){
New[i] <- new[i]
count[i] <- sum(Pcodes == new)
}
df3 <- data.frame(cbind(New, count))
print(df3)
View(df3)
View(df3)
View(df3)
#Question2: Show how often does each element in "new" occur in "Pcodes"
#...
New <- vector()
count <- vector()
for (i in 1:length(new)){
New[i] <- new[i]
count[i] <- sum(Pcodes == new)
}
#Question2: Show how often does each element in "new" occur in "Pcodes"
#...
New <- vector()
count <- vector()
for (i in 1:length(new)){
New[i] <- new[i]
count[i] <- sum(Pcodes == New)
}
df4 <- data.frame(cbind(New, count))
print(df4)
#Question2: Show how often does each element in "new" occur in "Pcodes"
#...
New <- vector()
count <- vector()
for (i in 1:length(new)){
New[i] <- new[i]
count[i] <- sum(Pcodes == new)
}
for (i in 1:length(new)){
New[i] <- new[i]
count[i] <- sum(Pcodes == New)
}
#Question2: Show how often does each element in "new" occur in "Pcodes"
#...
New <- vector()
count <- vector()
for (i in 1:length(new)){
New[i] <- new[i]
count[i] <- sum(Pcodes == New)
}
#Question2: Show how often does each element in "new" occur in "Pcodes"
#...
New <- vector()
count <- vector()
for (i in 1:length(new)){
New[i] <- new[i]
count[i] <- sum(Pcodes == new)
}
#Question2: Show how often does each element in "new" occur in "Pcodes"
#...
New <- vector()
count <- vector()
for (i in 1:length(new)){
New[i] <- new[i]
count[i] <- sum(Pcodes == New)
}
df3 <- data.frame(cbind(New, count))
print(df3)
#get all date of births (DOB)
dates = data$DOB
bdates <- as.Date(dates, "%d%B%Y");
ubdates = unique(bdates)
print(paste("Number of unique DOBs: ", length(ubdates)))
ubdates[order(as.Date(ubdates, format="%Y-%m-%d"))]
#Question: Does the list of DOBs contain outliers or noise points? Justify/explain your answer!
summary(bdates)
#Say we have a list of date+time in the format of the taxi-driver dataset
datelist = c('2014-02-01 00:00:24.6+01','2014-02-01 00:00:24.0+01','2014-02-12 02:10:14.8+01')
#R can convert this into standard form as follows:
dlstandard = as.POSIXlt(datelist)
#differences in time:
diff1 = dlstandard[1] - dlstandard[2]
diff2 = dlstandard[2] - dlstandard[3]
print(paste("First time difference:  ", diff1))
print(paste("Second time difference: ", diff2))
print(paste("Sum of time differences: ", diff1+diff2))
#Do you notice a problem here?
#Lets look at the two date objects
diff1;diff2
#Question: Solve the problem by using difftime() to ensure that both diff1 and diff2 are expressed in seconds.
#          Write down the difftime command(s) such that diff1 and diff2 are both expressed in seconds (inc. fraction of seconds)
diff1 = difftime(dlstandard[1],dlstandard[2],units = "secs")
diff2 = difftime(dlstandard[2],dlstandard[3],units = "secs")
#check that answer is correct:
print(paste("First time difference:  ", diff1))
print(paste("Second time difference: ", diff2))
print(paste("Sum of time differences: ", diff1+diff2))
diff1;diff2
# import library
library(randomForest)
library(data.table)
library(dplyr)
library(ranger)
library(caret)
library(unbalanced)
library(naivebayes)
library(rpart)
library(mice)
library(ggplot2)
library(VIM)
library(pROC)
library(RSNNS)
# load training data
data <- read.csv("aps_failure_training_set.csv", stringsAsFactors = FALSE, header = TRUE)
# Change class column into factor
data$class <- as.factor(data$class)
# Change column 2-171 into numeric and calculate % of missing value for each column
mv_rate<-c()# to store the index where NA > 5%
n<-1
for(i in 2:170){
data[,i]<-as.numeric(as.character(data[,i]))
mv<-sum(is.na(data[,i]))
rate<-mv/60000
if(rate>0.05){
mv_rate[n]<- i
n<-n+1
}
}
# Detect missing values
str(data)
print(sum(is.na(data)))
head(data, n=10)
# if NA > 5% delete the entire column
data <- data[-mv_rate] #(129)
# remove columns where over 90% of instance values are zero
col_zero <- lapply(data, function(col){length(which(col==0))/length(col)})
data <- data[, !(names(data) %in% names(col_zero[lapply(col_zero, function(x) x) > 0.9]))]
# visualize the missing value patterns
miss <- aggr(data)
# Change missing values to the mean of the column
for(i in 2:ncol(data)){
data[is.na(data[,i]), i] <- mean(data[,i], na.rm = TRUE)
}
# Checking missing values again
print(sum(is.na(data)))
# Visualize the ditribution of the dataset
qplot(as.factor(data$class), xlab = "class")
# Split dataset 60% train and 40 % test
set.seed(3456)
trainIndex <- createDataPartition(data$class, p = .6, list = FALSE, times = 1)
train <- data[ trainIndex,]
test <- data[-trainIndex,]
# Calculate total cost
total_cost <- function(result){
costs <- matrix(c(0, 10, 500, 0), 2)
return(sum(result * costs))
}
#-----------------------------Random Forest----------------------------------------------#
# Random forest model fitting
rf_classifier <- randomForest(class ~ ., data=train, ntree=100, mtry=2, importance=TRUE)
varImpPlot(rf_classifier)
plot(rf_classifier)
# Predict the class with Random Forest
rf_predict <- predict(rf_classifier, test)
test_cm <- table(rf_predict, test$class)
rf_result <- confusionMatrix(rf_predict, test$class)
rf_result
# Accuracy for Random Forest
rf_accuracy <- sum(diag(rf_result)/nrow(test))
cat("Random Forest accuracy : ", rf_accuracy)
rf_tc <- total_cost(rf_result)
cat("Random Forest total cost : ",rf_tc)
# load training data
data <- read.csv("aps_failure_training_set.csv", stringsAsFactors = FALSE, header = TRUE)
setwd("D:/IVAN/6th session/INFO411 - Data Mining and Knowledge Discovery/Project")
# load training data
data <- read.csv("aps_failure_training_set.csv", stringsAsFactors = FALSE, header = TRUE)
# Change class column into factor
data$class <- as.factor(data$class)
# Change column 2-171 into numeric and calculate % of missing value for each column
mv_rate<-c()# to store the index where NA > 5%
n<-1
for(i in 2:170){
data[,i]<-as.numeric(as.character(data[,i]))
mv<-sum(is.na(data[,i]))
rate<-mv/60000
if(rate>0.05){
mv_rate[n]<- i
n<-n+1
}
}
# Detect missing values
str(data)
print(sum(is.na(data)))
head(data, n=10)
# if NA > 5% delete the entire column
data <- data[-mv_rate] #(129)
# remove columns where over 90% of instance values are zero
col_zero <- lapply(data, function(col){length(which(col==0))/length(col)})
data <- data[, !(names(data) %in% names(col_zero[lapply(col_zero, function(x) x) > 0.9]))]
# visualize the missing value patterns
miss <- aggr(data)
# Change missing values to the mean of the column
for(i in 2:ncol(data)){
data[is.na(data[,i]), i] <- mean(data[,i], na.rm = TRUE)
}
# Checking missing values again
print(sum(is.na(data)))
# Visualize the ditribution of the dataset
qplot(as.factor(data$class), xlab = "class")
# Split dataset 60% train and 40 % test
set.seed(3456)
trainIndex <- createDataPartition(data$class, p = .6, list = FALSE, times = 1)
train <- data[ trainIndex,]
test <- data[-trainIndex,]
# Calculate total cost
total_cost <- function(result){
costs <- matrix(c(0, 10, 500, 0), 2)
return(sum(result * costs))
}
#-----------------------------Random Forest----------------------------------------------#
# Random forest model fitting
rf_classifier <- randomForest(class ~ ., data=train, ntree=100, mtry=2, importance=TRUE)
varImpPlot(rf_classifier)
plot(rf_classifier)
# Predict the class with Random Forest
rf_predict <- predict(rf_classifier, test)
test_cm <- table(rf_predict, test$class)
rf_result <- confusionMatrix(rf_predict, test$class)
rf_result
# Accuracy for Random Forest
rf_accuracy <- sum(diag(rf_result)/nrow(test))
cat("Random Forest accuracy : ", rf_accuracy)
rf_tc <- total_cost(rf_result)
cat("Random Forest total cost : ",rf_tc)
print(rf_classifier)
summary(rf_classifier)
recall(rf_result)
precision(rf_result)
recall <- recall(rf_result)
precision <- precision(rf_result)
F1 <- (2 * precision * recall) / (precision + recall)
F1
#naiveBayes
library(e1071)
library(cluster)
library(MASS)
nb<-naiveBayes(class~.,data=train)
pre<-predict(nb,test)
cm<-table(pre,test$class)
#confusion matrix
sum(diag(cm))/sum(cm)#acc:0.9659
#precision
cm[2,2]/(cm[2,1]+cm[2,2])#0.31
#recall
cm[2,2]/(cm[1,2]+cm[2,2])#0.8252
nb_tc <- total_cost(cm)
nb_tc
cm
precision(cm)
recall(cm)
#precision
rf_result[2,2]/(rf_result[2,1]+rf_result[2,2])#0.31
#recall
rf_result[2,2]/(rf_result[1,2]+rf_result[2,2])#0.8252
rf_result
#precision
precision <- rf_result[2,2]/(rf_result[2,1]+rf_result[2,2])#0.31
#recall
recall <- rf_result[2,2]/(rf_result[1,2]+rf_result[2,2])#0.8252
F1 <- (2 * precision * recall) / (precision + recall)
F1
#precision
rf_precision <- rf_result[2,2]/(rf_result[2,1]+rf_result[2,2])
#recall
rf_recall <- rf_result[2,2]/(rf_result[1,2]+rf_result[2,2])
#f1-score
rf_f1 <- (2 * rf_precision * rf_recall) / (rf_precision + rf_recall)
cat("Random Forest f1-score : ",rf_f1)
# Visualize the ditribution of the dataset
qplot(as.factor(data$class), xlab = "class")
plot(margin(rf_classifier,test$class))
plot(margin(rf_classifier,test$class))
rf_classifier$importance
plot(margin(rf_classifier))
dev.off()
plot(margin(rf_classifier))
as.numeric(unlist(rf_classifier))
plot(margin(rf_classifier))
# load training data
data <- read.csv("aps_failure_training_set.csv", stringsAsFactors = FALSE, header = TRUE)
# Change class column into factor
data$class <- as.factor(data$class)
# Change column 2-171 into numeric and calculate % of missing value for each column
mv_rate<-c()# to store the index where NA > 5%
n<-1
for(i in 2:170){
data[,i]<-as.numeric(as.character(data[,i]))
mv<-sum(is.na(data[,i]))
rate<-mv/60000
if(rate>0.05){
mv_rate[n]<- i
n<-n+1
}
}
# Detect missing values
str(data)
print(sum(is.na(data)))
head(data, n=10)
# if NA > 5% delete the entire column
data <- data[-mv_rate] #(129)
# remove columns where over 90% of instance values are zero
col_zero <- lapply(data, function(col){length(which(col==0))/length(col)})
data <- data[, !(names(data) %in% names(col_zero[lapply(col_zero, function(x) x) > 0.9]))]
# visualize the missing value patterns
miss <- aggr(data)
# Change missing values to the mean of the column
for(i in 2:ncol(data)){
data[is.na(data[,i]), i] <- mean(data[,i], na.rm = TRUE)
}
# Checking missing values again
print(sum(is.na(data)))
# Visualize the ditribution of the dataset
qplot(as.factor(data$class), xlab = "class")
# Split dataset 60% train and 40 % test
set.seed(3456)
trainIndex <- createDataPartition(data$class, p = .6, list = FALSE, times = 1)
train <- data[ trainIndex,]
test <- data[-trainIndex,]
# Calculate total cost
total_cost <- function(result){
costs <- matrix(c(0, 10, 500, 0), 2)
return(sum(result * costs))
}
#-----------------------------Random Forest----------------------------------------------#
# Random forest model fitting
rf_classifier <- randomForest(class ~ ., data=train, ntree=100, mtry=2, importance=TRUE)
plot(rf_classifier)
plot(margin(rf_classifier))
#-----------------------------Random Forest----------------------------------------------#
# Random forest model fitting
rf_classifier <- randomForest(class ~ ., data=train, ntree=100, mtry=2, importance=TRUE, keep.forest=FALSE)
plot(rf_classifier)
plot(margin(rf_classifier))
varImpPlot(rf_classifier)
result<- kmeans(train, 2)
# Fit decision tree to training dataset
dtree_classifier <- rpart(class ~., data = train, method = "class")
setwd("D:/IVAN/6th session/INFO411 - Data Mining and Knowledge Discovery/Project")
# import library
library(randomForest)
library(data.table)
library(dplyr)
library(ranger)
library(caret)
library(unbalanced)
library(naivebayes)
library(rpart)
library(mice)
library(ggplot2)
library(VIM)
library(pROC)
library(RSNNS)
# load training data
data <- read.csv("aps_failure_training_set.csv", stringsAsFactors = FALSE, header = TRUE)
# Change class column into factor
data$class <- as.factor(data$class)
# Change column 2-171 into numeric and calculate % of missing value for each column
mv_rate<-c()# to store the index where NA > 5%
n<-1
for(i in 2:170){
data[,i]<-as.numeric(as.character(data[,i]))
mv<-sum(is.na(data[,i]))
rate<-mv/60000
if(rate>0.05){
mv_rate[n]<- i
n<-n+1
}
}
# Detect missing values
str(data)
print(sum(is.na(data)))
head(data, n=10)
# if NA > 5% delete the entire column
data <- data[-mv_rate] #(129)
# remove columns where over 90% of instance values are zero
col_zero <- lapply(data, function(col){length(which(col==0))/length(col)})
data <- data[, !(names(data) %in% names(col_zero[lapply(col_zero, function(x) x) > 0.9]))]
# visualize the missing value patterns
miss <- aggr(data)
# Change missing values to the mean of the column
for(i in 2:ncol(data)){
data[is.na(data[,i]), i] <- mean(data[,i], na.rm = TRUE)
}
# Checking missing values again
print(sum(is.na(data)))
# Visualize the ditribution of the dataset
qplot(as.factor(data$class), xlab = "class")
# Split dataset 60% train and 40 % test
set.seed(3456)
trainIndex <- createDataPartition(data$class, p = .6, list = FALSE, times = 1)
train <- data[ trainIndex,]
test <- data[-trainIndex,]
# Calculate total cost
total_cost <- function(result){
costs <- matrix(c(0, 10, 500, 0), 2)
return(sum(result * costs))
}
# Fit decision tree to training dataset
dtree_classifier <- rpart(class ~., data = train, method = "class")
jpeg("dtree.jpeg", width = 800)
print(dtree_classifier)
plot(dtree_classifier, uniform=TRUE, margin=0.2)
text(dtree_classifier, use.n=TRUE, all=TRUE, cex=.9)
dt_predict <- predict(dtree_classifier, test, type = "class")
dt_result <- confusionMatrix(dt_predict, test$class)
dt_result
dt_tc <- total_cost(dt_result)
dt_tc
res.accuracy <- sum(diag(dt_result))/nrow(test)
res.accuracy
